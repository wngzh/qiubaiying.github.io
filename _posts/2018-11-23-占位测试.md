---
layout:     post
title:      Modern X86 CPU Technology
subtitle:   
date:       2018-11-23
author:     Wngzh
header-img: img/tag-bg.jpg
catalog: true
tags:
    - CPU
---

# 4004

&emsp;&emsp;X86_64的历史与改革开放同岁，从1978年开始，处理器发展就进入了一个新的时代，从专用处理芯片到通用微处理器时代。在1971年，Intel设计了世界上第一款微处理器，这块微处理器是为Busicom公司生产的芯片，当时Busicom为了生产计算器，向Intel定制了一套用于计算的芯片，12个，但是由于Intel工程师的建议，改为生产一套四个的芯片，名为MCS-4，这套系统有一个中央处理单元CPU-4004，一个ROM，以及一个RAM还有一个输入输出的控制芯片。通过对专利的回购，Intel生产出了世界上第一套可编程通用处理器。
&emsp;&emsp;4004仅仅能够寻址640B的空间，而且只能做加减乘除四种4bit的运算，因此在发布之后，业界反映平淡，直到8008的发布。

# 8008

&emsp;&emsp;8008是世界上第一款八位处理器，当时与Intel合作的公司是CTC，他们也想设计想4004的Busicom那样的使用集成电路片的计算机产品。这个直接造就了第一台真正意义上的微型计算机。8008可能是第一次开始有指令集设计的CPU，当时的指令集设计是由CTC公司完成的，硬件电路设计则是由IBM和Intel的工程师共同设计，这个指令集的汇编代码开启了向后兼容（backwards compatibility，新版本可以兼容老版产品），8008的指令集不仅仅可以在后续产品8080中对应，在直到今天无处不在的X86架构也可以对应实现。
&emsp;&emsp;8080可能是世界上第一款开始普及的CPU。8080和8008相比，名字来源可能是算力是8008的十倍。从这里开始，CPU工艺从10微米进化到了6微米，意味着越来越多的晶体管开始被集成到了芯片中，同时PMOS更换为了NMOS。虽然8080是一款8bit的CPU，但是已经具备了对16位数据进行操作的能力。有趣的是，8080开始，Intel开始成为了业界标准，Z80是有名的兼容者，它能够兼容8080的汇编代码，同时8080出现了大量的代工厂商，现在唯三的X86处理器制造商中，第二大的AMD就是靠着给Intel代工起家的。同时还有NEC、西门子、德州仪器这种现在看起来与商品级CPU完全无关的公司，甚至还有社会主义阵营这种苏联和波兰这种国家的产品。8080商业成功无处不在的影响了现代处理器的架构。有趣的是，有一个小行星的名称是8080 Intel，不知命名者意图如何，我觉得8086这个名字更配得上Intel。另外一件轶事是比尔盖茨和保罗艾伦在8080的时代为8080编写的basic语言解释器，开启了IT行业另一个巨头微软的发展史。
&emsp;&emsp;接下来就是40年经久不衰的X86架构开端8086了。X86就是得名于8086，8080的成功使得8086完全兼容了8080的汇编语言。作为大学的入门课程，计算机原理的8086和汇编语言是多少人的噩梦所在，但是汇编语言商业上的成功保证了直到今天，依旧还有很大的应用价值。

# 8086

&emsp;&emsp;8086作为第一款16位处理器，在1978年，支持了1M的内存空间寻址。同时还有数学处理器，用于浮点运算的8087。8086拥有AX, BX, CX, DX, SS, DS, CS, ES, IP, SI, DI, SP, BP这样的16位寄存器。此时的8086作为一款CPU产品，能够做的事情还很有限，诸如浮点运算、地址锁存（8282）、数据收发（8286）这样的工作还需要外部芯片来完成，根据外部链接芯片的多少，还分为最大和最小模式。仅仅拥有40个管脚，与现代的CPU不可同日而语。

# 386

&emsp;&emsp;到了80386世代，是xxx86最为辉煌的时刻，如今在Linux中区分64位或者32位操作系统就用的i386代指Intel的32位技术。80386拥有32位的寄存器和32位的地址线，32位的寄存器能够实现存储32bit的偏移量，编程时实现对多达4G的内存空间进行寻址。32位相对于16位来说，处理能力是提高了一倍，原本只能最多单次进行16个二进制的操作，现在能够对32bit进行运算。这对于CPU来说不仅从内存使用上到数据运算上来说，都是十分简单粗暴的性能提升方式。

# 486

&emsp;&emsp;到了486,486中开始引入了流水线（pipeline），在8086中，指令执行过程包括取指令，指令译码，指令执行，回写这样的过程，在指令执行的过程中，CPU其他部分处于等待状态不能进行其他操作，执行完成并回写后，才能够执行下一条操作。对于486来说，当进行完第一条指令预取后，马上进行第二条指令预取，此时第一条指令已经进入了指令解码阶段；第三条指令读取后，第二条进入指令解码，第一条进入了执行阶段...实际使用中，通过对流水线的更加细致的划分，可以使得每一步的操作变得简单，增加流水线长度，更容易达到更高的主频。这样原本需要多个指令周期才能够完成的指令，在理想情况下就能实现平均一个周期完成一条指令。最早流水线pipeline概念应用是在RISC处理器中，在MIPS、摩托罗拉88000中就使用这样的设计。但是往往超长的流水线会出现一些问题，在Pentium4 Prescott使用了31级的流水线设计，在2003年就实现了3.8GHz的主频，但是性能相当之差最后CEO出面下跪道歉，主要是在流水线级数增多的情况下，如果执行出错，将会付出很高的代价去修正错误。这里的出错并不是指计算错误，而是分支预测失误导致的指令回溯，流水线越长，付出的时钟周期浪费就越大，例如在jmp指令中，每次出现无条件跳转，往往就需要对流水线清空，重新读取指令队列。另外，流水线冒险是限制流水线效率的问题，流水线并不能够达到完全填满，会出现气泡（bubble），冒险分为这样几类，结构冒险，来自硬件的限制，硬件不足以支持这样几个同时的行为；数据冒险，指令之间的数据使用有依赖关系，执行先后顺序的不同会导致结果不同；控制冒险，对于某些控制逻辑来说，数据执行的改变，例如计数器，会导致执行的错误。为了避免冒险发生，流水线中就必须等待，不再提取新指令执行，就出现了Bubble，气泡，的情况。对于一些数据冒险，可以在每一级流水线之间使用锁存器来保存执行结果，然后使用一种转发的方式，将后级产生的结果通过旁路来送回前级需要的地方，相当于使用了执行结束的结果，减少等待周期，因此另外一个流水线过长带来的问题是，大量时间消耗在锁存器读写上，使得超流水线带来的频率提升变得不划算。

# Pentium

&emsp;&emsp;作为486的后继者，586已经不再按照原来的命名方式排列了，而是叫Pentium，这时候的Pentium已经有了Cache和集成在CPU内部的浮点运算单元，配备了新的指令集MMX，并且使用了超标量技术。超标量同时使用多条流水线实现指令上的并行，这个多条流水线能够并行的指令数目又被称为是发射数对于现代CPU来说，APPLE公司基于ARM架构的A8，能够实现6发射。在两条流水线同时运行与流水线同时运行多条指令有些类似，都涉及到了指令相关联带来的结果受执行顺序影响这样的问题。但是与流水线的冒险相比，因为超标量的实现是基于多套硬件结构，并不会出现硬件资源冲突的问题。对于超标量的判断，不仅要判断到数据和控制上的冒险操作，还要明确判断相关性，其中一种相关被称为是名称相关，是并行执行中两条指令对相同的寄存器进行读写造成的（但是数据上实际上不相关，也就是说在这两条指令间名称相关的部分没有数据流动），这里只需要使用不同的寄存器就可以避免这种情况，这样在软件上使用的办法是在编译器层面上的优化，对硬件层面，使用一种寄存器重命名的方法，对使用的寄存器进行重新命名达到避免名称冲突的效果。在超标量两条流水线运行中，必须通过停顿等方式的调度，来避免相关带来的影响。但是相对于RISC结构，RISC指令长度相同，执行时间也相同，判断执行的每一部分是否相关比X86容易的多，X86指令不等长、指令执行时间也不同，给并行执行判断相关性带来了极大的困难。对此Intel将执行中的X86指令分解为micro-ops（可能中文名是微指令、微程序），这是一种RISC风格的指令，这样就能实现指令级的并行。在Pentium中，流水线分为U、V两条。执行PF D1 D2 EX WB五个步骤，在D1的译码阶段如果取出的两条指令配对成功，就能够在U、V两条流水线上执行，如果不能够并行，则仅在U流水线执行。遇到指令受阻时，如果发生在U流水线，那么两条流水线全部退出，如果发生在V，则U流水线运行不受影响。
&emsp;&emsp;cache则是另一种提高CPU处理速度的思路，通过增加缓存（Cache），来提高对存储器的访问速度，减少读写造成的时间等待。因为随着CPU的发展，现在速度已经远远高于DRAM结构的主存储器，而对于SRAM来说，速度远远高于DRAM，但是成本非常高昂。对于SRAM来说，单位容量的价格大约是DRAM的数百倍，而DRAM的价格是磁盘的数百倍，速度是磁盘数十万倍。在程序执行过程中，适用于这样一种局部性原理：如果某个数据项被访问，在不久的时间之内将会被再次访问，并且在这个数据周围的数据项，往往与数据相关，也很与可能被用到。这分别被称为是时间的局部性和空间的局部性。利用这个原理，可以将计算机存储设备进行分级。最靠近CPU一级被是基于SRAM的cache，然后是DRAM组成的主存，最低一级则是磁盘。利用局部性原理，将可能用到的数据项提前从低级的慢速存储器搬移到高层次的存储器，就能实现提高存储器访问速度的效果。但是对于数据搬移的策略，涉及到命中率的问题。如果CPU能够在最高级的Cache中找到需要的数据，那么就实现了cache的命中，如果没有命中，则发生了缓存缺失，就要付出一定的缺失代价，需要耗费较多的时间将下层的数据逐级搬移到上层的cache中。一般来说现在的X86CPU中都有三级cache，典型的一个数值是L1Cache265KB，L2Cache1MB，L3Cache8MB，三级的命中率能够达到90%以上。逐级增大的Cache可以在上层Cache被使用时在下层制定一些策略来减少缓存缺失的发生。使用合理的Cache管理策略可以大大的提升CPU的读写效率。

![](/img/post/1-1.jpg)

&emsp;&emsp;Cache管理作为商业机密，很难获得现阶段的具体操作方法。但是